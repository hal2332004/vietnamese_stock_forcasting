import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime, timedelta
import time
import sys
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import os

# Danh s√°ch m√£ c·ªï phi·∫øu ng√¢n h√†ng
TICKERS = ["ACB", "BID", "VCB", "MBB", "FPT"]
# Kho·∫£ng th·ªùi gian l·∫•y tin (d√πng ƒë·ªÉ filter, kh√¥ng loop t·ª´ng ng√†y)
START_DATE = datetime(2015, 1, 1)
END_DATE = datetime(2025, 10, 30)

# Ngu·ªìn tin t·ª©c: cafef.vn (c√≥ th·ªÉ m·ªü r·ªông th√™m)
BASE_URL = "https://cafef.vn"
SEARCH_URL = "https://cafef.vn/tim-kiem.chn?query={query}&sfrom={sfrom}&sto={sto}&page={page}"

# C√°c query search ƒë·ªÉ l·∫•y nhi·ªÅu tin nh·∫•t
SEARCH_QUERIES = [
    "ng√¢n h√†ng",
    "ch·ª©ng kho√°n",
    "c·ªï phi·∫øu",
    "t√†i ch√≠nh",
]

# C·∫•u h√¨nh t·ªëi ∆∞u
MAX_WORKERS = 5  # S·ªë thread ƒë·ªìng th·ªùi
BATCH_SIZE = 50  # L∆∞u sau m·ªói 50 b√†i b√°o
MAX_RETRIES = 3  # S·ªë l·∫ßn th·ª≠ l·∫°i khi l·ªói
REQUEST_DELAY = 0.15  # Delay gi·ªØa c√°c request (gi√¢y)
MAX_PAGES_PER_QUERY = 50  # S·ªë trang t·ªëi ƒëa cho m·ªói query

# Thread-safe
csv_lock = Lock()
seen_urls = set()  # Tr√°nh tr√πng l·∫∑p

# H√†m crawl T·∫§T C·∫¢ tin t·ª©c - chia nh·ªè theo nƒÉm
def get_all_article_links(max_pages=MAX_PAGES_PER_QUERY):
    """
    Crawl T·∫§T C·∫¢ b√†i b√°o b·∫±ng c√°ch:
    1. Chia nh·ªè date range theo NƒÇM (2015, 2016, ..., 2025)
    2. V·ªõi m·ªói nƒÉm, d√πng nhi·ªÅu search queries
    3. Filter theo ticker trong n·ªôi dung
    """
    all_links = set()  # D√πng set ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i tr√πng
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://cafef.vn/',
    }
    
    # Chia theo NƒÇM
    start_year = START_DATE.year
    end_year = END_DATE.year
    
    print(f"\n[INFO] üîç Crawling by YEAR from {start_year} to {end_year}", file=sys.stderr)
    print(f"[INFO] Queries per year: {len(SEARCH_QUERIES)}", file=sys.stderr)
    print(f"[INFO] Target tickers: {', '.join(TICKERS)}", file=sys.stderr)
    print(f"[INFO] Max pages per query: {max_pages}", file=sys.stderr)
    
    # Loop qua t·ª´ng NƒÇM
    for year in range(start_year, end_year + 1):
        year_start = datetime(year, 1, 1)
        year_end = datetime(year, 12, 31) if year < end_year else END_DATE
        
        sfrom = year_start.strftime("%d/%m/%Y")
        sto = year_end.strftime("%d/%m/%Y")
        
        print(f"\n{'#'*70}", file=sys.stderr)
        print(f"[YEAR] üìÖ {year}: {sfrom} to {sto}", file=sys.stderr)
        print(f"{'#'*70}", file=sys.stderr)
        
        year_links_before = len(all_links)
        
        # V·ªõi m·ªói nƒÉm, d√πng nhi·ªÅu queries
        for query_idx, query in enumerate(SEARCH_QUERIES, 1):
            print(f"\n[INFO] Query {query_idx}/{len(SEARCH_QUERIES)}: '{query}'", file=sys.stderr)
            
            query_links = 0
            consecutive_empty = 0
            
            for page in range(1, max_pages + 1):
                # D·ª´ng n·∫øu 3 trang li√™n ti·∫øp kh√¥ng c√≥ link m·ªõi
                if consecutive_empty >= 3:
                    break
                    
                url = SEARCH_URL.format(query=query, sfrom=sfrom, sto=sto, page=page)
                
                try:
                    resp = requests.get(url, headers=headers, timeout=15)
                    
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        if consecutive_empty >= 3:
                            break
                        time.sleep(0.5)
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    
                    # T√¨m T·∫§T C·∫¢ article links
                    page_links = soup.find_all('a', href=True)
                    new_count = 0
                    
                    for a in page_links:
                        href = a.get('href', '')
                        
                        # Filter: CafeF article format
                        if '.chn' in href and '188' in href:
                            if not href.startswith('http'):
                                href = BASE_URL + href
                            
                            if href not in all_links:
                                all_links.add(href)
                                query_links += 1
                                new_count += 1
                    
                    if new_count == 0:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
            
            if query_links > 0:
                print(f"  ‚úÖ +{query_links} new links", file=sys.stderr)
        
        year_links_added = len(all_links) - year_links_before
        print(f"\n[YEAR {year}] ‚úÖ Total: +{year_links_added} links (cumulative: {len(all_links)})", file=sys.stderr)
    
    print(f"\n{'='*70}", file=sys.stderr)
    print(f"[SUCCESS] ‚úÖ Total: {len(all_links)} unique article links", file=sys.stderr)
    print(f"{'='*70}", file=sys.stderr)
    return list(all_links)

# H√†m parse date t·ª´ string sang ISO format
def parse_date(date_str):
    """
    Parse date t·ª´ nhi·ªÅu format kh√°c nhau v·ªÅ ISO format: YYYY-MM-DD HH:MM:SS
    """
    if not date_str:
        return ""
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    date_str = re.sub(r'\s+', ' ', date_str.strip())
    
    # Th·ª≠ c√°c format kh√°c nhau
    formats = [
        "%d/%m/%Y %H:%M",           # 30/10/2025 14:30
        "%d/%m/%Y - %H:%M",         # 30/10/2025 - 14:30
        "%d/%m/%Y %I:%M %p",        # 30/10/2025 02:30 PM
        "%d-%m-%Y %H:%M",           # 30-10-2025 14:30
        "%d/%m/%Y",                 # 30/10/2025
        "%Y-%m-%d %H:%M:%S",        # 2025-10-30 14:30:00 (already ISO)
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str, fmt)
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            continue
    
    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, tr·∫£ v·ªÅ string g·ªëc
    return date_str

# H√†m l·∫•y n·ªôi dung TO√ÄN B·ªò b√†i b√°o v·ªõi retry
def get_article_content(url, retries=MAX_RETRIES):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for attempt in range(retries):
        try:
            resp = requests.get(url, headers=headers, timeout=15)
            if resp.status_code != 200:
                print(f"[WARNING] HTTP {resp.status_code} for {url}", file=sys.stderr)
                if attempt < retries - 1:
                    time.sleep(2)
                    continue
                return None, None, None
                
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # ============= L·∫§Y TITLE =============
            title = ""
            title_selectors = [
                ".title-detail", ".detail-title", ".title",
                "h1.title", "h1", ".main-title",
                ".article-title", ".post-title"
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break
            
            # ============= L·∫§Y TO√ÄN B·ªò N·ªòI DUNG =============
            content = ""
            
            # Ph∆∞∆°ng ph√°p 1: T√¨m container ch√≠nh c·ªßa n·ªôi dung
            content_containers = [
                ".detail-content", ".main-content", ".content-detail",
                ".article-content", ".post-content", ".entry-content",
                "#mainContent", ".detail-contentmain", ".sapo-detail"
            ]
            
            content_found = False
            for container_selector in content_containers:
                container = soup.select_one(container_selector)
                if container:
                    # L·∫•y TO√ÄN B·ªò text t·ª´ container (bao g·ªìm c·∫£ sub-elements)
                    # Lo·∫°i b·ªè scripts, styles, ads
                    for unwanted in container.select("script, style, .ads, .advertisement, .related-news"):
                        unwanted.decompose()
                    
                    # L·∫•y t·∫•t c·∫£ text nodes
                    content_parts = []
                    
                    # ∆Øu ti√™n l·∫•y t·ª´ c√°c th·∫ª paragraph
                    paragraphs = container.select("p")
                    if paragraphs:
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text and len(text) > 20:  # L·ªçc c√°c ƒëo·∫°n qu√° ng·∫Øn
                                content_parts.append(text)
                    
                    # N·∫øu kh√¥ng c√≥ paragraph, l·∫•y to√†n b·ªô text t·ª´ container
                    if not content_parts:
                        full_text = container.get_text(separator="\n", strip=True)
                        # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng v√† d√≤ng qu√° ng·∫Øn
                        lines = [line.strip() for line in full_text.split("\n") if len(line.strip()) > 20]
                        content_parts.extend(lines)
                    
                    if content_parts:
                        content = " ".join(content_parts)
                        content_found = True
                        break
            
            # Ph∆∞∆°ng ph√°p 2: N·∫øu kh√¥ng t√¨m th·∫•y container, t√¨m tr·ª±c ti·∫øp c√°c paragraphs
            if not content_found:
                all_selectors = [
                    "article p", ".article p", ".post p",
                    ".content p", "main p", ".body p"
                ]
                for selector in all_selectors:
                    paragraphs = soup.select(selector)
                    if paragraphs:
                        content_parts = []
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text and len(text) > 20:
                                content_parts.append(text)
                        if content_parts:
                            content = " ".join(content_parts)
                            content_found = True
                            break
            
            # ============= L·∫§Y DATE/TIME =============
            date_str = ""
            date_selectors = [
                ".date", ".pdate", ".time", ".publish-time",
                ".post-time", ".date-time", "time", ".article-date",
                ".timeago", "[datetime]"
            ]
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    # Th·ª≠ l·∫•y t·ª´ attribute datetime tr∆∞·ªõc
                    date_str = date_elem.get('datetime', '') or date_elem.get_text(strip=True)
                    if date_str:
                        break
            
            # Parse date sang ISO format
            parsed_date = parse_date(date_str)
            
            # ============= VALIDATION =============
            # Y√™u c·∫ßu n·ªôi dung t·ªëi thi·ªÉu 100 k√Ω t·ª±
            if len(content) < 100:
                print(f"[WARNING] Content too short ({len(content)} chars) from {url}", file=sys.stderr)
                if attempt < retries - 1:
                    time.sleep(2)
                    continue
            
            if not title:
                print(f"[WARNING] No title found for {url}", file=sys.stderr)
            
            if not content:
                print(f"[WARNING] No content found for {url}", file=sys.stderr)
                if attempt < retries - 1:
                    time.sleep(2)
                    continue
            
            # Log ƒë·ªô d√†i content ƒë·ªÉ debug
            if content:
                print(f"[INFO] Extracted {len(content)} chars from {url[:50]}...", file=sys.stderr)
                
            return title, content, parsed_date
            
        except Exception as e:
            if attempt < retries - 1:
                print(f"[WARNING] Retry {attempt+1}/{retries} for {url}: {e}", file=sys.stderr)
                time.sleep(2)
            else:
                print(f"[ERROR] Failed to get content from {url}: {e}", file=sys.stderr)
                import traceback
                traceback.print_exc(file=sys.stderr)
                return None, None, None
    
    return None, None, None

# H√†m l∆∞u batch v√†o CSV (thread-safe)
def save_batch_to_csv(batch, output_file, write_header=False):
    with csv_lock:
        mode = 'w' if write_header else 'a'
        with open(output_file, mode, encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=["date", "time", "title", "content", "ticker", "source"])
            if write_header:
                writer.writeheader()
            for row in batch:
                writer.writerow(row)

# H√†m detect ticker n√†o ƒë∆∞·ª£c mention trong b√†i b√°o
def detect_tickers_in_content(title, content):
    """
    Ph√°t hi·ªán ticker n√†o ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong title/content
    Returns: list of tickers found
    """
    text = (title + " " + content).upper()
    found_tickers = []
    
    for ticker in TICKERS:
        # Check ticker name v√† c√°c bi·∫øn th·ªÉ
        ticker_patterns = [
            ticker,  # VD: "ACB"
            f" {ticker} ",  # Word boundary
            f"({ticker})",  # In parentheses
            f"{ticker},",  # With comma
        ]
        
        if any(pattern in text for pattern in ticker_patterns):
            found_tickers.append(ticker)
    
    return found_tickers

# H√†m x·ª≠ l√Ω 1 article URL (KH√îNG filter ticker ·ªü ƒë√¢y)
def process_article(url):
    # Ki·ªÉm tra duplicate
    if url in seen_urls:
        return []
    
    seen_urls.add(url)
    
    title, content, date_str = get_article_content(url)
    
    # Y√™u c·∫ßu n·ªôi dung t·ªëi thi·ªÉu
    if not content or len(content) < 100:
        return []
    
    # T√°ch date v√† time
    try:
        if date_str and len(date_str) >= 10:
            if ' ' in date_str:
                date_part, time_part = date_str.split(' ', 1)
            else:
                date_part = date_str[:10]
                time_part = ""
        else:
            date_part = date_str
            time_part = ""
    except:
        date_part = date_str
        time_part = ""
    
    # N·∫øu kh√¥ng c√≥ title, d√πng 50 k√Ω t·ª± ƒë·∫ßu
    if not title:
        title = content[:50] + "..." if len(content) > 50 else content
    
    # PH√ÅT HI·ªÜN ticker n√†o ƒë∆∞·ª£c mention
    mentioned_tickers = detect_tickers_in_content(title, content)
    
    if not mentioned_tickers:
        return []
    
    # T·∫°o 1 record cho M·ªñI ticker ƒë∆∞·ª£c mention
    results = []
    for ticker in mentioned_tickers:
        results.append({
            "date": date_part,
            "time": time_part,
            "title": title,
            "content": content,
            "ticker": ticker,
            "source": url
        })
    
    if results:
        tickers_str = ", ".join(mentioned_tickers)
        print(f"[INFO] ‚úÖ Article mentions: {tickers_str} | {title[:50]}...", file=sys.stderr)
    
    return results

# H√†m crawl to√†n b·ªô d·ªØ li·ªáu (NEW APPROACH - crawl ALL then filter)
def crawl_news(output_file):
    batch = []
    
    # Kh·ªüi t·∫°o file CSV
    save_batch_to_csv([], output_file, write_header=True)
    
    print("\n" + "="*70, file=sys.stderr)
    print(f"[STEP 1] üîç CRAWLING ALL ARTICLE LINKS", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    # L·∫•y T·∫§T C·∫¢ links (kh√¥ng filter ticker)
    all_links = get_all_article_links(max_pages=MAX_PAGES_PER_QUERY)
    
    if not all_links:
        print("[ERROR] ‚ùå No links found!", file=sys.stderr)
        return 0
    
    print("\n" + "="*70, file=sys.stderr)
    print(f"[STEP 2] üìù PROCESSING {len(all_links)} ARTICLES", file=sys.stderr)
    print(f"[INFO] Filtering by tickers: {', '.join(TICKERS)}", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    # X·ª≠ l√Ω song song T·∫§T C·∫¢ links
    total_records = 0
    ticker_stats = {ticker: 0 for ticker in TICKERS}
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_article, url): url for url in all_links}
        
        completed = 0
        for future in as_completed(futures):
            completed += 1
            
            if completed % 50 == 0:
                print(f"[PROGRESS] Processed {completed}/{len(all_links)} articles ({total_records} records saved)", file=sys.stderr)
            
            try:
                results = future.result()  # Can return multiple records
                
                if results:
                    for result in results:
                        batch.append(result)
                        total_records += 1
                        ticker_stats[result['ticker']] += 1
                        
                        # L∆∞u batch
                        if len(batch) >= BATCH_SIZE:
                            save_batch_to_csv(batch, output_file)
                            print(f"[SAVE] ‚úÖ Saved batch of {len(batch)} records. Total: {total_records}", file=sys.stderr)
                            batch = []
                            
            except Exception as e:
                print(f"[ERROR] {e}", file=sys.stderr)
    
    # L∆∞u batch cu·ªëi
    if batch:
        save_batch_to_csv(batch, output_file)
        print(f"\n[SAVE] ‚úÖ Saved final batch of {len(batch)} records", file=sys.stderr)
    
    # Th·ªëng k√™
    print("\n" + "="*70, file=sys.stderr)
    print("üìä STATISTICS BY TICKER:", file=sys.stderr)
    for ticker in TICKERS:
        count = ticker_stats[ticker]
        print(f"  {ticker}: {count:>5} articles", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    return total_records

if __name__ == "__main__":
    print("="*70)
    print("üöÄ VIETNAMESE STOCK NEWS CRAWLER - OPTIMIZED VERSION")
    print("="*70)
    print(f"[INFO] Date range: {START_DATE.date()} to {END_DATE.date()}", file=sys.stderr)
    print(f"[INFO] Tickers: {', '.join(TICKERS)}", file=sys.stderr)
    print(f"[INFO] Max workers: {MAX_WORKERS}, Batch size: {BATCH_SIZE}", file=sys.stderr)
    
    # T√™n file output
    output_file = f"news_fulltext_{START_DATE.year}_{END_DATE.year}.csv"
    
    # Ki·ªÉm tra file ƒë√£ t·ªìn t·∫°i
    if os.path.exists(output_file):
        print(f"\n[WARNING] File {output_file} ƒë√£ t·ªìn t·∫°i!")
        choice = input("Overwrite? (y/n): ")
        if choice.lower() != 'y':
            print("[INFO] Crawl cancelled by user")
            sys.exit(0)
    
    # B·∫Øt ƒë·∫ßu crawl
    start_time = time.time()
    print(f"\n[START] Crawling started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    try:
        total = crawl_news(output_file)
        
        elapsed = time.time() - start_time
        print("\n" + "="*70)
        print(f"[SUCCESS] ‚úÖ ƒê√£ l∆∞u {total} b√†i b√°o v√†o {output_file}")
        print(f"[TIME] ‚è±Ô∏è  Th·ªùi gian: {elapsed/60:.2f} ph√∫t ({elapsed:.1f} gi√¢y)")
        print(f"[SPEED] üöÑ T·ªëc ƒë·ªô: {total/(elapsed/60):.1f} b√†i/ph√∫t")
        print("="*70)
        
        # Th·ªëng k√™ theo ticker
        if os.path.exists(output_file):
            with open(output_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                ticker_counts = {}
                for row in reader:
                    ticker = row['ticker']
                    ticker_counts[ticker] = ticker_counts.get(ticker, 0) + 1
            
            print("\nüìä Th·ªëng k√™ theo m√£ c·ªï phi·∫øu:")
            for ticker in TICKERS:
                count = ticker_counts.get(ticker, 0)
                print(f"  {ticker}: {count:>5} b√†i b√°o")
        
    except KeyboardInterrupt:
        print("\n[INFO] ‚ö†Ô∏è  Crawl b·ªã ng·∫Øt b·ªüi ng∆∞·ªùi d√πng")
        print(f"[INFO] D·ªØ li·ªáu ƒë√£ crawl ƒë∆∞·ª£c l∆∞u trong {output_file}")
    except Exception as e:
        print(f"\n[ERROR] ‚ùå L·ªói: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
