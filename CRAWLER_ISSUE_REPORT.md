# V·∫§N ƒê·ªÄ V·ªöI CRAWLER C≈® V√Ä GI·∫¢I PH√ÅP

## üî¥ **V·∫•n ƒë·ªÅ ph√°t hi·ªán:**

### 1. **Ch·∫°y l√¢u nh∆∞ng √≠t b√†i vi·∫øt**
- Ch·∫°y **65 ph√∫t** (3933 gi√¢y) nh∆∞ng ch·ªâ c√≥ **1253 articles**
- T·ªëc ƒë·ªô: 19.1 articles/ph√∫t ‚Üí qu√° ch·∫≠m

### 2. **Ph√¢n b·ªë nƒÉm kh√¥ng ƒë·ªÅu**
```
2015: 1253 articles (ACB: 144, BID: 195, VCB: 116, MBB: 13, FPT: 782)
2016-2023: 0 articles
2024: 3 articles (ACB only)
2025: 0 articles
```

### 3. **CSV c√≥ nhi·ªÅu nƒÉm r·∫£i r√°c**
Trong file `news_MBB_2015_2025.csv`:
- C√≥ b√†i t·ª´ **2014, 2020, 2021, 2025**
- Date b·ªã parse sai: "Th·ª© ba, 15/7/2014, 17:45 (GMT+7)" ‚Üí l∆∞u v√†o CSV l√† "Th·ª©"
- Ch·ªâ c√≥ **14 d√≤ng** trong CSV d√π crawler b√°o crawled 1063 articles cho MBB 2018

### 4. **Ticker detection sai**
- Query "MB" cho MBB nh∆∞ng match c·∫£ "Qu√¢n ƒë·ªôi" (Military Bank)
- V√≠ d·ª•: Nhi·ªÅu b√†i v·ªÅ "Qu√¢n ƒë·ªôi Israel", "Qu√¢n ƒë·ªôi M·ªπ", "Qu√¢n ƒë·ªôi Ukraine" ƒë∆∞·ª£c detect l√† MBB
- Pattern matching qu√° loose: `"MB" in text` ‚Üí match v·ªõi "MB Bank", "MBB", "qu√¢n ƒë·ªôi M·ªπ"

---

## ‚úÖ **Gi·∫£i ph√°p trong `multi_source_crawler_fixed.py`:**

### 1. **Fix Date Parsing**
```python
def parse_date(date_str):
    # X√≥a "Th·ª© hai, Th·ª© ba,..." 
    date_str = re.sub(r'Th·ª©\s+(hai|ba|t∆∞|nƒÉm|s√°u|b·∫£y|CN),?\s*', '', date_str, flags=re.IGNORECASE)
    # X√≥a (GMT+7)
    date_str = re.sub(r'\(GMT\+\d+\)', '', date_str)
    
    # Parse v·ªõi multiple formats
    formats = ["%d/%m/%Y, %H:%M", "%d/%m/%Y %H:%M", ...]
    
    # Fallback: extract year b·∫±ng regex
    year_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', date_str)
```

**K·∫øt qu·∫£ test:**
```
Input:  Th·ª© ba, 15/7/2014, 17:45 (GMT+7)
Output: 2014-07-15 17:45:00  ‚úÖ

Input:  Invalid date string
Output: (empty) ‚Üí B·ªè qua b√†i n√†y ‚úÖ
```

### 2. **Strict Year Validation**
```python
# Skip if cannot parse date
if not parsed_date or len(parsed_date) < 10:
    return None

# Skip if cannot extract year
article_year = extract_year_from_date(parsed_date)
if not article_year:
    return None

# Skip if wrong year
if article_year != target_year:
    return None
```

### 3. **Flexible Ticker Detection**
```python
TICKER_FULL_NAMES = {
    "ACB": ["ACB", "√Å Ch√¢u", "Asia Commercial Bank", "ng√¢n h√†ng ACB", "NH ACB"],
    "BID": ["BID", "BIDV", "ƒê·∫ßu t∆∞ v√† Ph√°t tri·ªÉn", ...],
    "VCB": ["VCB", "Vietcombank", "Ngo·∫°i th∆∞∆°ng", ...],
    "MBB": ["MBB", "MB Bank", "MB", "ng√¢n h√†ng MB", "Military Bank", "Qu√¢n ƒê·ªôi", "NH Qu√¢n ƒê·ªôi"],
    "FPT": ["FPT", "FPT Corporation", "T·∫≠p ƒëo√†n FPT", ...],
}

def detect_ticker_in_content(title, content, ticker):
    # Check ALL possible names
    possible_names = TICKER_FULL_NAMES.get(ticker, [ticker])
    for name in possible_names:
        if name.upper() in text:
            return True
```

**V·∫•n ƒë·ªÅ c√≤n t·ªìn t·∫°i**: MBB v·∫´n c√≥ th·ªÉ match v·ªõi "Qu√¢n ƒë·ªôi" trong context kh√°c
‚Üí C·∫ßn th√™m **negative keywords** ƒë·ªÉ filter:

```python
NEGATIVE_KEYWORDS = {
    "MBB": ["qu√¢n ƒë·ªôi Israel", "qu√¢n ƒë·ªôi M·ªπ", "qu√¢n ƒë·ªôi Ukraine", "qu√¢n ƒë·ªôi Nga", 
            "qu√¢n ƒë·ªôi Myanmar", "qu√¢n ƒë·ªôi Syria", "qu√¢n ƒë·ªôi Trung Qu·ªëc"]
}
```

### 4. **Per-Ticker seen_urls**
```python
# OLD: Global seen_urls ‚Üí duplicate URLs b·ªã b·ªè qua gi·ªØa c√°c ticker
seen_urls = set()

# NEW: Separate per ticker
for ticker in TICKERS:
    seen_urls_for_ticker = set()  # Ri√™ng cho t·ª´ng ticker
    process_article(..., seen_urls_for_ticker)
```

### 5. **Lower Content Threshold**
```python
# OLD: 100 chars minimum
if not content or len(content) < 100:
    return None

# NEW: 50 chars minimum
if not content or len(content) < 50:
    return None
```

---

## üìä **K·∫øt qu·∫£ d·ª± ki·∫øn sau fix:**

1. ‚úÖ Date parsing ch√≠nh x√°c ‚Üí ƒê√∫ng nƒÉm trong CSV
2. ‚úÖ Year validation ‚Üí B·ªè qua b√†i sai nƒÉm
3. ‚úÖ Per-ticker URLs ‚Üí Kh√¥ng b·ªè qua b√†i tr√πng gi·ªØa ticker
4. ‚úÖ Flexible detection ‚Üí Catch ƒë∆∞·ª£c "√Å Ch√¢u", "Vietcombank", "MB Bank"
5. ‚ö†Ô∏è V·∫´n c·∫ßn th√™m negative keywords cho MBB

---

## üéØ **Khuy·∫øn ngh·ªã:**

### Ngay l·∫≠p t·ª©c:
1. ‚úÖ Ch·∫°y `multi_source_crawler_fixed.py` thay v√¨ `multi_source_crawler.py`
2. ‚úÖ X√≥a c√°c file CSV c≈© ƒë·ªÉ crawl l·∫°i t·ª´ ƒë·∫ßu

### C·∫£i ti·∫øn th√™m:
1. Th√™m **negative keywords** ƒë·ªÉ filter b√†i kh√¥ng li√™n quan
2. Th√™m **positive context** (v√≠ d·ª•: "l·ª£i nhu·∫≠n", "c·ªï phi·∫øu", "t√≠n d·ª•ng")
3. Crawl **song song theo nƒÉm** thay v√¨ tu·∫ßn t·ª± (s·ª≠ d·ª•ng multiprocessing)
4. Th√™m **retry logic** cho failed requests

### Monitoring:
```bash
# Theo d√µi progress
tail -f crawler_output.log | grep "‚úÖ"

# Check s·ªë l∆∞·ª£ng theo nƒÉm
bash check_progress.sh
```

---

## üìù **L·ªánh ch·∫°y:**

```bash
cd /mnt/d/Ky\ 4/financial-news-sentiment-main/Source/recode

# X√≥a CSV c≈©
rm news_*_2015_2025.csv multi_source_news_2015_2025.csv

# Ch·∫°y crawler fixed
nohup python multi_source_crawler_fixed.py > crawler_fixed.log 2>&1 &

# Ho·∫∑c test v·ªõi 1-2 nƒÉm tr∆∞·ªõc
# S·ª≠a START_DATE = datetime(2024, 1, 1) trong code
```
