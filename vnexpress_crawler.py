import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime, timedelta
import time
import sys
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import os

# Danh s√°ch m√£ c·ªï phi·∫øu
TICKERS = ["ACB", "BID", "VCB", "MBB", "FPT"]

# Kho·∫£ng th·ªùi gian l·∫•y tin
START_DATE = datetime(2015, 1, 1)
END_DATE = datetime(2025, 10, 30)

# VnExpress URLs
BASE_URL = "https://vnexpress.net"
SEARCH_URL = "https://timkiem.vnexpress.net/?q={query}&date_from={date_from}&date_to={date_to}&media_type=all&cate_code=&latest=&page={page}"

# C√°c query search
SEARCH_QUERIES = [
    "ng√¢n h√†ng",
    "ch·ª©ng kho√°n",
    "c·ªï phi·∫øu",
    "ACB",
    "BID", 
    "VCB",
    "MBB",
    "FPT",
]

# C·∫•u h√¨nh
MAX_WORKERS = 5
BATCH_SIZE = 50
MAX_RETRIES = 3
REQUEST_DELAY = 0.3
MAX_PAGES_PER_QUERY = 50

# Thread-safe
csv_lock = Lock()
seen_urls = set()

# H√†m crawl links t·ª´ VnExpress search - chia theo NƒÇM
def get_all_article_links(max_pages=MAX_PAGES_PER_QUERY):
    """
    Crawl T·∫§T C·∫¢ b√†i b√°o t·ª´ VnExpress
    Chia theo NƒÇM ƒë·ªÉ l·∫•y nhi·ªÅu k·∫øt qu·∫£
    """
    all_links = set()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://vnexpress.net/',
    }
    
    start_year = START_DATE.year
    end_year = END_DATE.year
    
    print(f"\n[INFO] üîç Crawling VnExpress from {start_year} to {end_year}", file=sys.stderr)
    print(f"[INFO] Queries: {len(SEARCH_QUERIES)}", file=sys.stderr)
    print(f"[INFO] Target tickers: {', '.join(TICKERS)}", file=sys.stderr)
    
    # Loop qua t·ª´ng NƒÇM
    for year in range(start_year, end_year + 1):
        year_start = datetime(year, 1, 1)
        year_end = datetime(year, 12, 31) if year < end_year else END_DATE
        
        date_from = year_start.strftime("%Y-%m-%d")
        date_to = year_end.strftime("%Y-%m-%d")
        
        print(f"\n{'#'*70}", file=sys.stderr)
        print(f"[YEAR] üìÖ {year}: {date_from} to {date_to}", file=sys.stderr)
        print(f"{'#'*70}", file=sys.stderr)
        
        year_links_before = len(all_links)
        
        # V·ªõi m·ªói nƒÉm, d√πng nhi·ªÅu queries
        for query in SEARCH_QUERIES:
            print(f"\n[INFO] Query: '{query}'", file=sys.stderr)
            
            query_links = 0
            consecutive_empty = 0
            
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 3:
                    break
                
                url = SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    date_from=date_from,
                    date_to=date_to,
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=15)
                    
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        if consecutive_empty >= 3:
                            break
                        time.sleep(1)
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    
                    # VnExpress: articles trong <h3 class="title-news">
                    articles = soup.find_all('h3', class_='title-news')
                    
                    new_count = 0
                    for article in articles:
                        a_tag = article.find('a', href=True)
                        if a_tag:
                            href = a_tag.get('href', '')
                            
                            # Construct full URL
                            if href.startswith('http'):
                                full_url = href
                            elif href.startswith('/'):
                                full_url = BASE_URL + href
                            else:
                                continue
                            
                            # Check duplicate
                            if full_url not in all_links:
                                all_links.add(full_url)
                                query_links += 1
                                new_count += 1
                    
                    if new_count == 0:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    print(f"[ERROR] Page {page}: {e}", file=sys.stderr)
                    consecutive_empty += 1
                    time.sleep(1)
            
            if query_links > 0:
                print(f"  ‚úÖ +{query_links} new links", file=sys.stderr)
        
        year_links_added = len(all_links) - year_links_before
        print(f"\n[YEAR {year}] ‚úÖ Total: +{year_links_added} links (cumulative: {len(all_links)})", file=sys.stderr)
    
    print(f"\n{'='*70}", file=sys.stderr)
    print(f"[SUCCESS] ‚úÖ Total: {len(all_links)} unique article links", file=sys.stderr)
    print(f"{'='*70}", file=sys.stderr)
    return list(all_links)

# H√†m parse date
def parse_date(date_str):
    if not date_str:
        return ""
    
    date_str = re.sub(r'\s+', ' ', date_str.strip())
    
    formats = [
        "%d/%m/%Y, %H:%M",
        "%d/%m/%Y %H:%M",
        "%Y-%m-%d %H:%M:%S",
        "%d/%m/%Y",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str, fmt)
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            continue
    
    return date_str

# H√†m l·∫•y n·ªôi dung b√†i b√°o VnExpress
def get_article_content(url, retries=MAX_RETRIES):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    for attempt in range(retries):
        try:
            resp = requests.get(url, headers=headers, timeout=15)
            if resp.status_code != 200:
                if attempt < retries - 1:
                    time.sleep(2)
                    continue
                return None, None, None
                
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # ============= TITLE =============
            title = ""
            title_selectors = [
                "h1.title-detail",
                "h1.title_news_detail",
                "h1",
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break
            
            # ============= CONTENT =============
            content = ""
            
            # VnExpress content containers
            content_selectors = [
                "article.fck_detail",
                ".fck_detail",
                "article.content_detail",
                ".content_detail",
            ]
            
            for selector in content_selectors:
                container = soup.select_one(selector)
                if container:
                    # Remove unwanted elements
                    for unwanted in container.select("script, style, .box_brief_info, .box_category, .ads"):
                        unwanted.decompose()
                    
                    # Get all paragraphs
                    paragraphs = container.select("p.Normal")
                    if not paragraphs:
                        paragraphs = container.select("p")
                    
                    if paragraphs:
                        content_parts = []
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text and len(text) > 20:
                                content_parts.append(text)
                        content = " ".join(content_parts)
                        break
            
            # ============= DATE =============
            date_str = ""
            date_selectors = [
                "span.date",
                ".date",
                "time",
                "[datetime]",
            ]
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_str = date_elem.get('datetime', '') or date_elem.get_text(strip=True)
                    if date_str:
                        break
            
            parsed_date = parse_date(date_str)
            
            # Validation
            if len(content) < 100:
                if attempt < retries - 1:
                    time.sleep(2)
                    continue
            
            if content:
                print(f"[INFO] Extracted {len(content)} chars from {url[:50]}...", file=sys.stderr)
                
            return title, content, parsed_date
            
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(2)
            else:
                print(f"[ERROR] Failed: {url}: {e}", file=sys.stderr)
                return None, None, None
    
    return None, None, None

# H√†m detect tickers trong content
def detect_tickers_in_content(title, content):
    text = (title + " " + content).upper()
    found_tickers = []
    
    for ticker in TICKERS:
        patterns = [
            ticker,
            f" {ticker} ",
            f"({ticker})",
            f"{ticker},",
        ]
        
        if any(pattern in text for pattern in patterns):
            found_tickers.append(ticker)
    
    return found_tickers

# H√†m x·ª≠ l√Ω 1 article
def process_article(url):
    if url in seen_urls:
        return []
    
    seen_urls.add(url)
    
    title, content, date_str = get_article_content(url)
    
    if not content or len(content) < 100:
        return []
    
    # Parse date/time
    try:
        if date_str and len(date_str) >= 10:
            if ' ' in date_str:
                date_part, time_part = date_str.split(' ', 1)
            else:
                date_part = date_str[:10]
                time_part = ""
        else:
            date_part = date_str
            time_part = ""
    except:
        date_part = date_str
        time_part = ""
    
    if not title:
        title = content[:50] + "..." if len(content) > 50 else content
    
    # Detect tickers
    mentioned_tickers = detect_tickers_in_content(title, content)
    
    if not mentioned_tickers:
        return []
    
    # Create records
    results = []
    for ticker in mentioned_tickers:
        results.append({
            "date": date_part,
            "time": time_part,
            "title": title,
            "content": content,
            "ticker": ticker,
            "source": url
        })
    
    if results:
        tickers_str = ", ".join(mentioned_tickers)
        print(f"[INFO] ‚úÖ Article mentions: {tickers_str} | {title[:50]}...", file=sys.stderr)
    
    return results

# H√†m l∆∞u batch
def save_batch_to_csv(batch, output_file, write_header=False):
    with csv_lock:
        mode = 'w' if write_header else 'a'
        with open(output_file, mode, encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=["date", "time", "title", "content", "ticker", "source"])
            if write_header:
                writer.writeheader()
            for row in batch:
                writer.writerow(row)

# H√†m crawl ch√≠nh
def crawl_news(output_file):
    batch = []
    
    # Init file
    save_batch_to_csv([], output_file, write_header=True)
    
    print("\n" + "="*70, file=sys.stderr)
    print(f"[STEP 1] üîç CRAWLING ALL ARTICLE LINKS FROM VNEXPRESS", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    all_links = get_all_article_links(max_pages=MAX_PAGES_PER_QUERY)
    
    if not all_links:
        print("[ERROR] ‚ùå No links found!", file=sys.stderr)
        return 0
    
    print("\n" + "="*70, file=sys.stderr)
    print(f"[STEP 2] üìù PROCESSING {len(all_links)} ARTICLES", file=sys.stderr)
    print(f"[INFO] Filtering by tickers: {', '.join(TICKERS)}", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    total_records = 0
    ticker_stats = {ticker: 0 for ticker in TICKERS}
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_article, url): url for url in all_links}
        
        completed = 0
        for future in as_completed(futures):
            completed += 1
            
            if completed % 50 == 0:
                print(f"[PROGRESS] Processed {completed}/{len(all_links)} articles ({total_records} records saved)", file=sys.stderr)
            
            try:
                results = future.result()
                
                if results:
                    for result in results:
                        batch.append(result)
                        total_records += 1
                        ticker_stats[result['ticker']] += 1
                        
                        if len(batch) >= BATCH_SIZE:
                            save_batch_to_csv(batch, output_file)
                            print(f"[SAVE] ‚úÖ Saved batch of {len(batch)} records. Total: {total_records}", file=sys.stderr)
                            batch = []
                            
            except Exception as e:
                print(f"[ERROR] {e}", file=sys.stderr)
    
    # Save final batch
    if batch:
        save_batch_to_csv(batch, output_file)
        print(f"\n[SAVE] ‚úÖ Saved final batch of {len(batch)} records", file=sys.stderr)
    
    # Statistics
    print("\n" + "="*70, file=sys.stderr)
    print("üìä STATISTICS BY TICKER:", file=sys.stderr)
    for ticker in TICKERS:
        count = ticker_stats[ticker]
        print(f"  {ticker}: {count:>5} articles", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    return total_records

if __name__ == "__main__":
    print("="*70)
    print("üöÄ VNEXPRESS NEWS CRAWLER")
    print("="*70)
    print(f"[INFO] Date range: {START_DATE.date()} to {END_DATE.date()}", file=sys.stderr)
    print(f"[INFO] Tickers: {', '.join(TICKERS)}", file=sys.stderr)
    print(f"[INFO] Max workers: {MAX_WORKERS}, Batch size: {BATCH_SIZE}", file=sys.stderr)
    
    output_file = f"vnexpress_news_{START_DATE.year}_{END_DATE.year}.csv"
    
    if os.path.exists(output_file):
        print(f"\n[WARNING] File {output_file} ƒë√£ t·ªìn t·∫°i!")
        choice = input("Overwrite? (y/n): ")
        if choice.lower() != 'y':
            print("[INFO] Crawl cancelled by user")
            sys.exit(0)
    
    start_time = time.time()
    print(f"\n[START] Crawling started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    try:
        total = crawl_news(output_file)
        
        elapsed = time.time() - start_time
        print("\n" + "="*70)
        print(f"[SUCCESS] ‚úÖ ƒê√£ l∆∞u {total} b√†i b√°o v√†o {output_file}")
        print(f"[TIME] ‚è±Ô∏è  Th·ªùi gian: {elapsed/60:.2f} ph√∫t ({elapsed:.1f} gi√¢y)")
        print(f"[SPEED] üöÑ T·ªëc ƒë·ªô: {total/(elapsed/60):.1f} b√†i/ph√∫t")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n[INFO] ‚ö†Ô∏è  Crawl b·ªã ng·∫Øt b·ªüi ng∆∞·ªùi d√πng")
        print(f"[INFO] D·ªØ li·ªáu ƒë√£ crawl ƒë∆∞·ª£c l∆∞u trong {output_file}")
    except Exception as e:
        print(f"\n[ERROR] ‚ùå L·ªói: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
