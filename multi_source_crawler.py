"""
MULTI-SOURCE NEWS CRAWLER
Crawl tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß d·ªØ li·ªáu cho 5 tickers trong 10 nƒÉm
Sources: VnExpress, D√¢n Tr√≠, CafeF
"""

import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime, timedelta
import time
import sys
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import os

# ============= CONFIGURATION =============
TICKERS = ["BID", "FPT"]
START_DATE = datetime(2015, 1, 1)  # Full range 2015-2025
END_DATE = datetime(2025, 10, 30)

MAX_WORKERS = 5
BATCH_SIZE = 100
MAX_RETRIES = 3
REQUEST_DELAY = 0.2

# Thread-safe
csv_lock = Lock()
seen_urls = set()

# ============= VNEXPRESS CRAWLER =============
class VnExpressCrawler:
    BASE_URL = "https://vnexpress.net"
    SEARCH_URL = "https://timkiem.vnexpress.net/?q={query}&date_from={date_from}&date_to={date_to}&media_type=all&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=50):
        """Crawl article links t·ª´ VnExpress theo ticker v√† nƒÉm - NHI·ªÄU QUERIES"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        year_start = datetime(year, 1, 1)
        year_end = datetime(year, 12, 31) if year < END_DATE.year else END_DATE
        
        # T√™n ƒë·∫ßy ƒë·ªß c·ªßa c√°c ng√¢n h√†ng
        ticker_names = {
            "ACB": ["ACB", "√Å Ch√¢u", "ng√¢n h√†ng ACB", "Asia Commercial Bank"],
            "BID": ["BID", "BIDV", "ƒê·∫ßu t∆∞ v√† Ph√°t tri·ªÉn", "ng√¢n h√†ng BIDV"],
            "VCB": ["VCB", "Vietcombank", "ng√¢n h√†ng Vietcombank", "Ngo·∫°i th∆∞∆°ng"],
            "MBB": ["MBB", "MB Bank", "ng√¢n h√†ng MB", "Military Bank"],
            "FPT": ["FPT", "FPT Corporation", "T·∫≠p ƒëo√†n FPT", "c·ªï phi·∫øu FPT"],
        }
        
        # T·∫°o queries T·∫¨P TRUNG V√ÄO T√ÄI CH√çNH
        base_queries = ticker_names.get(ticker, [ticker])
        queries = []
        for name in base_queries:
            queries.extend([
                # T√†i ch√≠nh c·ª• th·ªÉ
                f"{name} b√°o c√°o t√†i ch√≠nh",
                f"{name} k·∫øt qu·∫£ kinh doanh",
                f"{name} l·ª£i nhu·∫≠n",
                f"{name} doanh thu",
                f"{name} b√°o c√°o qu√Ω",
            ])
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = VnExpressCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    date_from=year_start.strftime("%Y-%m-%d"),
                    date_to=year_end.strftime("%Y-%m-%d"),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.find_all('h3', class_='title-news')
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    for article in articles:
                        a_tag = article.find('a', href=True)
                        if a_tag:
                            href = a_tag.get('href', '')
                            if href.startswith('http'):
                                links.append(('vnexpress', href))
                            elif href.startswith('/'):
                                links.append(('vnexpress', VnExpressCrawler.BASE_URL + href))
                    
                    consecutive_empty = 0
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract title, content, date t·ª´ VnExpress article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # Title
            title = ""
            title_elem = soup.select_one("h1.title-detail")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # Content
            content = ""
            content_elem = soup.select_one("article.fck_detail")
            if content_elem:
                paragraphs = content_elem.select("p.Normal")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            # Date
            date_str = ""
            date_elem = soup.select_one("span.date")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= D√ÇN TR√ç CRAWLER =============
class DanTriCrawler:
    BASE_URL = "https://dantri.com.vn"
    SEARCH_URL = "https://dantri.com.vn/tim-kiem.htm?q={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=50):
        """Crawl article links t·ª´ D√¢n Tr√≠ - NHI·ªÄU QUERIES"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        ticker_names = {
            "ACB": ["ACB", "√Å Ch√¢u", "Asia Commercial Bank"],
            "BID": ["BID", "BIDV", "ng√¢n h√†ng BIDV"],
            "VCB": ["VCB", "Vietcombank", "ng√¢n h√†ng Vietcombank"],
            "MBB": ["MBB", "MB Bank", "ng√¢n h√†ng MB"],
            "FPT": ["FPT", "FPT Corporation", "T·∫≠p ƒëo√†n FPT"],
        }
        
        base_queries = ticker_names.get(ticker, [ticker])
        queries = []
        for name in base_queries:
            queries.extend([
                f"{name} b√°o c√°o t√†i ch√≠nh",
                f"{name} k·∫øt qu·∫£ kinh doanh",
                f"{name} l·ª£i nhu·∫≠n",
                f"{name} doanh thu",
                f"{name} b√°o c√°o qu√Ω",
            ])
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = DanTriCrawler.SEARCH_URL.format(query=query.replace(' ', '+'), page=page)
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    
                    # D√¢n Tr√≠ search results
                    articles = soup.select("h3.article-title a, h4.article-title a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        # Check if article is from target year
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('dantri', href))
                            elif href.startswith('/'):
                                links.append(('dantri', DanTriCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ D√¢n Tr√≠ article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # Title
            title = ""
            title_elem = soup.select_one("h1.title-page, h1.article-title")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # Content
            content = ""
            content_elem = soup.select_one("div.singular-content, div.article-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            # Date
            date_str = ""
            date_elem = soup.select_one("time.author-time, span.author-time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= THANHNIEN CRAWLER =============
class ThanhNienCrawler:
    BASE_URL = "https://thanhnien.vn"
    SEARCH_URL = "https://thanhnien.vn/tim-kiem/?keywords={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ ThanhNien.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        ticker_names = {
            "ACB": ["ACB", "ng√¢n h√†ng ACB"],
            "BID": ["BIDV", "ng√¢n h√†ng BIDV"],
            "VCB": ["Vietcombank", "ng√¢n h√†ng Vietcombank"],
            "MBB": ["MB Bank", "ng√¢n h√†ng MB"],
            "FPT": ["FPT", "FPT Corporation"],
        }
        
        queries = ticker_names.get(ticker, [ticker])
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = ThanhNienCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    
                    # ThanhNien search results
                    articles = soup.select("h2.title-news a, h3.title-news a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        # Check if from target year
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('thanhnien', href))
                            elif href.startswith('/'):
                                links.append(('thanhnien', ThanhNienCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ ThanhNien article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # Title
            title = ""
            title_elem = soup.select_one("h1.detail-title, h1.title-detail")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # Content
            content = ""
            content_elem = soup.select_one("div.detail-content, div#contentbody")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            # Date
            date_str = ""
            date_elem = soup.select_one("div.detail-time, time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= CAFEF CRAWLER =============
class CafeFCrawler:
    BASE_URL = "https://cafef.vn"
    SEARCH_URL = "https://cafef.vn/tim-kiem.chn?keywords={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=20):
        """Crawl t·ª´ CafeF - FINANCIAL FOCUSED"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        # Financial-focused queries
        queries = [
            f"{ticker} b√°o c√°o t√†i ch√≠nh",
            f"{ticker} k·∫øt qu·∫£ kinh doanh",
            f"{ticker} l·ª£i nhu·∫≠n",
            ticker  # Fallback to ticker only
        ]
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 3:
                    break
                
                url = CafeFCrawler.SEARCH_URL.format(query=query, page=page)
            
            try:
                resp = requests.get(url, headers=headers, timeout=10)
                if resp.status_code != 200:
                    consecutive_empty += 1
                    continue
                
                soup = BeautifulSoup(resp.text, "html.parser")
                all_links = soup.find_all('a', href=True)
                
                found_articles = False
                for a in all_links:
                    href = a.get('href', '')
                    text = a.get_text(strip=True).lower()
                    
                    # Check if link contains news article pattern and mentions ticker
                    if '.chn' in href and (ticker.lower() in text or ticker.lower() in href.lower()):
                        # Check if from correct year
                        if str(year) in href or f'{year % 100:02d}' in href:
                            found_articles = True
                            if not href.startswith('http'):
                                href = CafeFCrawler.BASE_URL + href
                            links.append(('cafef', href))
                
                if not found_articles:
                    consecutive_empty += 1
                else:
                    consecutive_empty = 0
                
                time.sleep(REQUEST_DELAY)
                
            except Exception as e:
                consecutive_empty += 1
                time.sleep(0.5)
        
        return links

# ============= VIETSTOCK CRAWLER =============
class VietstockCrawler:
    BASE_URL = "https://finance.vietstock.vn"
    SEARCH_URL = "https://finance.vietstock.vn/tim-kiem?keyword={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ Vietstock.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        ticker_names = {
            "ACB": ["ACB", "√Å Ch√¢u"],
            "BID": ["BID", "BIDV"],
            "VCB": ["VCB", "Vietcombank"],
            "MBB": ["MBB", "MB Bank"],
            "FPT": ["FPT", "FPT Corporation"],
        }
        
        queries = ticker_names.get(ticker, [ticker])
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = VietstockCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("h3 a, h2.news-title a, div.news-item a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('vietstock', href))
                            elif href.startswith('/'):
                                links.append(('vietstock', VietstockCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ Vietstock article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            title = ""
            title_elem = soup.select_one("h1.news-title, h1.detail-title")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            content = ""
            content_elem = soup.select_one("div.detail-content, div.news-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            date_str = ""
            date_elem = soup.select_one("span.time, div.date")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= STOCKBIZ CRAWLER =============
class StockbizCrawler:
    BASE_URL = "https://stockbiz.vn"
    SEARCH_URL = "https://stockbiz.vn/tim-kiem.html?q={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ Stockbiz.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        ticker_names = {
            "ACB": ["ACB", "ng√¢n h√†ng ACB"],
            "BID": ["BIDV", "ng√¢n h√†ng BIDV"],
            "VCB": ["Vietcombank"],
            "MBB": ["MB Bank"],
            "FPT": ["FPT"],
        }
        
        queries = ticker_names.get(ticker, [ticker])
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = StockbizCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("h3 a, h2 a, div.article-item a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('stockbiz', href))
                            elif href.startswith('/'):
                                links.append(('stockbiz', StockbizCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ Stockbiz article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            title = ""
            title_elem = soup.select_one("h1.title, h1")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            content = ""
            content_elem = soup.select_one("div.content, div.article-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            date_str = ""
            date_elem = soup.select_one("span.date, time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= NDH CRAWLER =============
class NDHCrawler:
    BASE_URL = "https://ndh.vn"
    SEARCH_URL = "https://ndh.vn/tim-kiem?key={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ ndh.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        queries = [ticker]
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = NDHCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("h3 a, h2 a, div.news-item a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('ndh', href))
                            elif href.startswith('/'):
                                links.append(('ndh', NDHCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ NDH article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            title = ""
            title_elem = soup.select_one("h1.title, h1")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            content = ""
            content_elem = soup.select_one("div.content, div.detail-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            date_str = ""
            date_elem = soup.select_one("span.date, time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= TINNHANHCHUNGKHOAN CRAWLER =============
class TinnhanhchungkhoanCrawler:
    BASE_URL = "https://tinnhanhchungkhoan.vn"
    SEARCH_URL = "https://tinnhanhchungkhoan.vn/search?q={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ Tinnhanhchungkhoan.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        queries = [ticker]
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = TinnhanhchungkhoanCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("h3 a, h2.title a, div.article a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('tinnhanhchungkhoan', href))
                            elif href.startswith('/'):
                                links.append(('tinnhanhchungkhoan', TinnhanhchungkhoanCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ Tinnhanhchungkhoan article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            title = ""
            title_elem = soup.select_one("h1.title, h1")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            content = ""
            content_elem = soup.select_one("div.content, div.detail-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            date_str = ""
            date_elem = soup.select_one("span.date, time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= BAODAUTU CRAWLER =============
class BaodautuCrawler:
    BASE_URL = "https://baodautu.vn"
    SEARCH_URL = "https://baodautu.vn/tim-kiem.html?q={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ baodautu.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        ticker_names = {
            "ACB": ["ACB", "√Å Ch√¢u"],
            "BID": ["BIDV"],
            "VCB": ["Vietcombank"],
            "MBB": ["MB Bank"],
            "FPT": ["FPT"],
        }
        
        queries = ticker_names.get(ticker, [ticker])
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = BaodautuCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("h3 a, h2.title a, div.news-item a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('baodautu', href))
                            elif href.startswith('/'):
                                links.append(('baodautu', BaodautuCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ Baodautu article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            title = ""
            title_elem = soup.select_one("h1.title, h1")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            content = ""
            content_elem = soup.select_one("div.content, div.detail-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            date_str = ""
            date_elem = soup.select_one("span.date, time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= VIETFINANCE CRAWLER =============
class VietFinanceCrawler:
    BASE_URL = "https://vietfinance.vn"
    SEARCH_URL = "https://vietfinance.vn/tim-kiem?keyword={query}&page={page}"
    
    @staticmethod
    def get_article_links(ticker, year, max_pages=30):
        """Crawl t·ª´ VietFinance.vn"""
        links = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        queries = [ticker]
        
        for query in queries:
            consecutive_empty = 0
            for page in range(1, max_pages + 1):
                if consecutive_empty >= 2:
                    break
                
                url = VietFinanceCrawler.SEARCH_URL.format(
                    query=query.replace(' ', '+'),
                    page=page
                )
                
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        consecutive_empty += 1
                        continue
                    
                    soup = BeautifulSoup(resp.text, "html.parser")
                    articles = soup.select("h3 a, h2 a, div.article a")
                    
                    if not articles:
                        consecutive_empty += 1
                        continue
                    
                    found_year_match = False
                    for article in articles:
                        href = article.get('href', '')
                        
                        if str(year) in href or f"/{year % 100:02d}/" in href:
                            found_year_match = True
                            if href.startswith('http'):
                                links.append(('vietfinance', href))
                            elif href.startswith('/'):
                                links.append(('vietfinance', VietFinanceCrawler.BASE_URL + href))
                    
                    if not found_year_match:
                        consecutive_empty += 1
                    else:
                        consecutive_empty = 0
                    
                    time.sleep(REQUEST_DELAY)
                    
                except Exception as e:
                    consecutive_empty += 1
                    time.sleep(0.5)
        
        return links
    
    @staticmethod
    def extract_content(url):
        """Extract content t·ª´ VietFinance article"""
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, None, None
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            title = ""
            title_elem = soup.select_one("h1.title, h1")
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            content = ""
            content_elem = soup.select_one("div.content, div.detail-content")
            if content_elem:
                paragraphs = content_elem.select("p")
                if paragraphs:
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            date_str = ""
            date_elem = soup.select_one("span.date, time")
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return title, content, date_str
            
        except Exception as e:
            return None, None, None

# ============= MAIN CRAWLER =============
def parse_date(date_str):
    """Parse date to ISO format"""
    if not date_str:
        return ""
    
    date_str = re.sub(r'\s+', ' ', date_str.strip())
    
    formats = [
        "%d/%m/%Y, %H:%M",
        "%d/%m/%Y %H:%M",
        "%Y-%m-%d %H:%M:%S",
        "%d/%m/%Y",
        "%d-%m-%Y",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str, fmt)
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            continue
    
    return date_str

# ============= FINANCIAL KEYWORDS FILTER =============
FINANCIAL_KEYWORDS = {
    "common": [
        # B√°o c√°o t√†i ch√≠nh
        "b√°o c√°o t√†i ch√≠nh", "k·∫øt qu·∫£ kinh doanh", "b√°o c√°o qu√Ω", "b√°o c√°o nƒÉm",
        "financial report", "quarterly", "annual report", "Q1", "Q2", "Q3", "Q4",
        
        # L·ª£i nhu·∫≠n & Doanh thu
        "l·ª£i nhu·∫≠n", "doanh thu", "tƒÉng tr∆∞·ªüng", "EPS", "ROE", "ROA",
        "profit", "revenue", "earnings", "growth", "t·ª∑ ƒë·ªìng", "ngh√¨n t·ª∑",
        
        # V·ªën & C·ªï phi·∫øu
        "v·ªën h√≥a", "c·ªï phi·∫øu", "c·ªï ƒë√¥ng", "ph√°t h√†nh", "chia c·ªï t·ª©c", "gi√° c·ªï phi·∫øu",
        "market cap", "shares", "shareholder", "dividend", "stock price",
        
        # Giao d·ªãch l·ªõn
        "mua l·∫°i", "s√°p nh·∫≠p", "M&A", "h·ª£p ƒë·ªìng", "th∆∞∆°ng v·ª•", "ƒë·∫ßu t∆∞",
        "acquisition", "merger", "deal", "contract", "investment",
        
        # Ph√¢n t√≠ch & D·ª± b√°o
        "ƒë·ªãnh gi√°", "m·ª•c ti√™u", "khuy·∫øn ngh·ªã", "tri·ªÉn v·ªçng", "d·ª± b√°o", "ph√¢n t√≠ch",
        "valuation", "target", "recommendation", "outlook", "forecast", "analysis",
        
        # Th·ªã tr∆∞·ªùng & Vƒ© m√¥
        "FTSE", "n√¢ng h·∫°ng", "upgrade", "downgrade", "rating",
        "v·ªën ngo·∫°i", "foreign", "institutional", "VN-Index", "HOSE", "HNX",
    ],
    
    "BID": [
        # Ng√¢n h√†ng c·ª• th·ªÉ
        "t√≠n d·ª•ng", "n·ª£ x·∫•u", "NPL", "huy ƒë·ªông", "cho vay", "ti·ªÅn g·ª≠i",
        "credit", "bad debt", "loan", "deposit", "lending",
        
        # Ch·ªâ s·ªë ng√¢n h√†ng
        "t·ª∑ l·ªá an to√†n v·ªën", "CAR", "Basel", "NIM", "l√£i su·∫•t", "interest rate",
        "d·ª± ph√≤ng", "provision", "CIR", "chi ph√≠ ho·∫°t ƒë·ªông",
        
        # Ho·∫°t ƒë·ªông ng√¢n h√†ng
        "t·ªïng t√†i s·∫£n", "v·ªën ch·ªß s·ªü h·ªØu", "l√£i thu·∫ßn", "thu nh·∫≠p l√£i",
    ],
    
    "FPT": [
        # C√¥ng ngh·ªá & D·ªãch v·ª•
        "h·ª£p ƒë·ªìng", "chuy·ªÉn ƒë·ªïi s·ªë", "digital transformation",
        "AI", "cloud", "outsourcing", "ph·∫ßn m·ªÅm", "software",
        
        # C√°c c√¥ng ty con
        "FPT Telecom", "FPT Software", "FPT IS", "FPT Retail", "Long Ch√¢u",
        "vi·ªÖn th√¥ng", "telecom", "b√°n l·∫ª", "retail",
        
        # M·ªü r·ªông
        "xu·∫•t kh·∫©u", "export", "overseas", "qu·ªëc t·∫ø", "international",
        "Nh·∫≠t B·∫£n", "Japan", "ASEAN", "Singapore", "M·ªπ",
        
        # D·ªãch v·ª•
        "d·ªãch v·ª• s·ªë", "c√¥ng ngh·ªá", "technology", "IT services",
    ]
}

EXCLUDE_KEYWORDS = [
    # Tin h√†nh ch√≠nh nh·ªè kh√¥ng quan tr·ªçng
    "khai tr∆∞∆°ng", "chi nh√°nh m·ªõi", "vƒÉn ph√≤ng m·ªõi", "thay ƒë·ªïi ƒë·ªãa ch·ªâ",
    "opening ceremony", "new branch", "new office",
    
    # Tin s·ª± ki·ªán x√£ h·ªôi
    "t·ª´ thi·ªán", "charity", "CSR", "tr√°ch nhi·ªám x√£ h·ªôi",
    "tuy·ªÉn d·ª•ng", "recruitment", "hiring", "tuy·ªÉn sinh",
    
    # Tin qu·∫£ng c√°o marketing
    "khuy·∫øn m√£i", "promotion", "sale", "gi·∫£m gi√°", "∆∞u ƒë√£i",
    "ra m·∫Øt s·∫£n ph·∫©m", "new product launch" # tr·ª´ khi l√† s·∫£n ph·∫©m t√†i ch√≠nh l·ªõn
]

def check_financial_relevance(title, content, ticker):
    """
    Ki·ªÉm tra xem tin c√≥ li√™n quan ƒë·∫øn T√ÄI CH√çNH kh√¥ng
    Returns: (is_relevant, score, matched_keywords)
    """
    text = (title + " " + content[:1500]).upper()  # Ch·ªâ check 1500 k√Ω t·ª± ƒë·∫ßu
    
    # 1. Check exclude keywords tr∆∞·ªõc (lo·∫°i b·ªè tin kh√¥ng quan tr·ªçng)
    for exclude_word in EXCLUDE_KEYWORDS:
        if exclude_word.upper() in text:
            return False, 0, []
    
    # 2. Count matched financial keywords
    matched_keywords = []
    score = 0
    
    # Common financial keywords (tr·ªçng s·ªë 1)
    for keyword in FINANCIAL_KEYWORDS["common"]:
        if keyword.upper() in text:
            matched_keywords.append(keyword)
            score += 1
    
    # Ticker-specific keywords (tr·ªçng s·ªë 2)
    if ticker in FINANCIAL_KEYWORDS:
        for keyword in FINANCIAL_KEYWORDS[ticker]:
            if keyword.upper() in text:
                matched_keywords.append(keyword)
                score += 2  # Keywords ƒë·∫∑c th√π c√≥ tr·ªçng s·ªë cao h∆°n
    
    # 3. Bonus n·∫øu c√≥ s·ªë li·ªáu c·ª• th·ªÉ
    if any(pattern in text for pattern in ["T·ª∂ ƒê·ªíNG", "NGH√åN T·ª∂", "TRI·ªÜU USD", "MILLION", "BILLION"]):
        score += 1
    
    # 4. Check ticker mention
    if ticker.upper() not in text:
        score = int(score * 0.3)  # Gi·∫£m m·∫°nh score n·∫øu kh√¥ng nh·∫Øc ticker
    
    # 5. Threshold: c·∫ßn √≠t nh·∫•t 2 ƒëi·ªÉm
    is_relevant = score >= 2
    
    return is_relevant, score, matched_keywords[:5]  # Top 5 keywords

def detect_ticker_in_content(title, content, ticker):
    """Check if ticker is mentioned in content AND financially relevant"""
    # Check basic mention
    text = (title + " " + content).upper()
    
    patterns = [
        ticker,
        f" {ticker} ",
        f"({ticker})",
        f"{ticker},",
        f"{ticker}.",
    ]
    
    has_ticker = any(pattern in text for pattern in patterns)
    
    if not has_ticker:
        return False
    
    # Check financial relevance
    is_relevant, score, keywords = check_financial_relevance(title, content, ticker)
    
    return is_relevant

def process_article(source, url, ticker):
    """Process single article from any source"""
    if url in seen_urls:
        return None
    
    seen_urls.add(url)
    
    # Extract content based on source
    if source == 'vnexpress':
        title, content, date_str = VnExpressCrawler.extract_content(url)
    elif source == 'dantri':
        title, content, date_str = DanTriCrawler.extract_content(url)
    elif source == 'thanhnien':
        title, content, date_str = ThanhNienCrawler.extract_content(url)
    elif source == 'cafef':
        title, content, date_str = extract_cafef_content(url)
    elif source == 'vietstock':
        title, content, date_str = VietstockCrawler.extract_content(url)
    elif source == 'stockbiz':
        title, content, date_str = StockbizCrawler.extract_content(url)
    elif source == 'ndh':
        title, content, date_str = NDHCrawler.extract_content(url)
    elif source == 'tinnhanhchungkhoan':
        title, content, date_str = TinnhanhchungkhoanCrawler.extract_content(url)
    elif source == 'baodautu':
        title, content, date_str = BaodautuCrawler.extract_content(url)
    elif source == 'vietfinance':
        title, content, date_str = VietFinanceCrawler.extract_content(url)
    else:
        return None
    
    # Validate
    if not content or len(content) < 100:
        return None
    
    if not detect_ticker_in_content(title or "", content, ticker):
        return None
    
    # Parse date
    parsed_date = parse_date(date_str)
    
    try:
        if parsed_date and len(parsed_date) >= 10:
            if ' ' in parsed_date:
                date_part, time_part = parsed_date.split(' ', 1)
            else:
                date_part = parsed_date[:10]
                time_part = ""
        else:
            date_part = parsed_date
            time_part = ""
    except:
        date_part = parsed_date
        time_part = ""
    
    if not title:
        title = content[:50] + "..."
    
    print(f"[INFO] ‚úÖ {source.upper()}: {ticker} | {title[:50]}...", file=sys.stderr)
    
    return {
        "date": date_part,
        "time": time_part,
        "title": title,
        "content": content,
        "ticker": ticker,
        "source": f"{source}:{url}"
    }

def extract_cafef_content(url):
    """Extract content from CafeF (backup source)"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code != 200:
            return None, None, None
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        title = ""
        title_elem = soup.select_one(".title-detail, h1")
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        content = ""
        content_elem = soup.select_one(".detail-content, .main-content")
        if content_elem:
            paragraphs = content_elem.select("p")
            if paragraphs:
                content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
        
        date_str = ""
        date_elem = soup.select_one(".date, time")
        if date_elem:
            date_str = date_elem.get_text(strip=True)
        
        return title, content, date_str
    except:
        return None, None, None

def save_batch_to_csv(batch, output_file, write_header=False):
    """Save batch to CSV (thread-safe) - SINGLE FILE"""
    with csv_lock:
        # Check if file exists to determine if header needed
        file_exists = os.path.exists(output_file)
        
        mode = 'a'
        with open(output_file, mode, encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=["date", "time", "title", "content", "ticker", "source"])
            if not file_exists or write_header:
                writer.writeheader()
            for row in batch:
                writer.writerow(row)

def crawl_multi_source(output_file):
    """Main crawler - crawl t·ª´ nhi·ªÅu ngu·ªìn - SAVE TO SINGLE FILE"""
    batch = []
    
    # Remove old file if exists
    if os.path.exists(output_file):
        os.remove(output_file)
    
    total_records = 0
    ticker_year_stats = {}
    
    print("\n" + "="*70, file=sys.stderr)
    print("üåê MULTI-SOURCE NEWS CRAWLER", file=sys.stderr)
    print("="*70, file=sys.stderr)
    print(f"[INFO] Sources: VnExpress (80 pages), CafeF (50 pages)", file=sys.stderr)
    print(f"[INFO] Note: Only tested working sources included", file=sys.stderr)
    print(f"[INFO] Period: {START_DATE.year}-{END_DATE.year}", file=sys.stderr)
    print(f"[INFO] Tickers: {', '.join(TICKERS)}", file=sys.stderr)
    print(f"[INFO] Target: 250+ articles/ticker/year", file=sys.stderr)
    print(f"[INFO] Output: Single CSV file ‚Üí {output_file}", file=sys.stderr)
    
    # Crawl theo t·ª´ng NƒÇM v√† TICKER
    for year in range(START_DATE.year, END_DATE.year + 1):
        print(f"\n{'#'*70}", file=sys.stderr)
        print(f"[YEAR] üìÖ {year}", file=sys.stderr)
        print(f"{'#'*70}", file=sys.stderr)
        
        for ticker in TICKERS:
            print(f"\n[{year}] üíº Ticker: {ticker}", file=sys.stderr)
            
            # Collect links from all sources
            all_links = []
            
            # VnExpress (80 pages - primary source)
            print(f"  üì∞ Crawling VnExpress...", file=sys.stderr)
            vnexpress_links = VnExpressCrawler.get_article_links(ticker, year, max_pages=80)
            all_links.extend(vnexpress_links)
            print(f"    ‚úÖ Found {len(vnexpress_links)} links", file=sys.stderr)
            
            # CafeF (50 pages - secondary source)
            print(f"  üì∞ Crawling CafeF...", file=sys.stderr)
            cafef_links = CafeFCrawler.get_article_links(ticker, year, max_pages=50)
            all_links.extend(cafef_links)
            print(f"    ‚úÖ Found {len(cafef_links)} links", file=sys.stderr)
            
            if not all_links:
                print(f"  ‚ö†Ô∏è  No articles found for {ticker} in {year}", file=sys.stderr)
                continue
            
            print(f"  üîÑ Processing {len(all_links)} articles...", file=sys.stderr)
            
            # Process articles
            ticker_year_count = 0
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = {executor.submit(process_article, source, url, ticker): (source, url) for source, url in all_links}
                
                for future in as_completed(futures):
                    try:
                        result = future.result()
                        if result:
                            batch.append(result)
                            total_records += 1
                            ticker_year_count += 1
                            
                            if len(batch) >= BATCH_SIZE:
                                save_batch_to_csv(batch, output_file)
                                print(f"[SAVE] ‚úÖ Saved {len(batch)} records. Total: {total_records}", file=sys.stderr)
                                batch = []
                    except Exception as e:
                        pass
            
            ticker_year_stats[f"{year}_{ticker}"] = ticker_year_count
            print(f"  ‚úÖ {ticker} {year}: {ticker_year_count} articles", file=sys.stderr)
    
    # Save final batch
    if batch:
        save_batch_to_csv(batch, output_file)
        print(f"\n[SAVE] ‚úÖ Saved final batch of {len(batch)} records", file=sys.stderr)
    
    # Print summary
    print("\n" + "="*70, file=sys.stderr)
    print("üìä SUMMARY BY YEAR AND TICKER:", file=sys.stderr)
    print("="*70, file=sys.stderr)
    
    for year in range(START_DATE.year, END_DATE.year + 1):
        print(f"\n{year}:", file=sys.stderr)
        for ticker in TICKERS:
            count = ticker_year_stats.get(f"{year}_{ticker}", 0)
            print(f"  {ticker}: {count:>4} articles", file=sys.stderr)
    
    return total_records

if __name__ == "__main__":
    print("="*70)
    print("üåê MULTI-SOURCE VIETNAMESE STOCK NEWS CRAWLER")
    print("="*70)
    print(f"[INFO] Date range: {START_DATE.date()} to {END_DATE.date()}", file=sys.stderr)
    print(f"[INFO] Tickers: {', '.join(TICKERS)}", file=sys.stderr)
    
    # Create data folder if not exists
    data_folder = "data"
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
    
    output_file = os.path.join(data_folder, f"news_{START_DATE.year}_{END_DATE.year}.csv")
    
    # Check if file exists
    if os.path.exists(output_file):
        print(f"\n[WARNING] Found existing file: {output_file}")
        choice = input("Overwrite? (y/n): ")
        if choice.lower() != 'y':
            print("[INFO] Crawl cancelled")
            sys.exit(0)
    
    start_time = time.time()
    print(f"\n[START] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    try:
        total = crawl_multi_source(output_file)
        
        elapsed = time.time() - start_time
        print("\n" + "="*70)
        print(f"[SUCCESS] ‚úÖ Saved {total} articles")
        print(f"[TIME] ‚è±Ô∏è  Duration: {elapsed/60:.2f} minutes ({elapsed:.1f} seconds)")
        print(f"[SPEED] üöÑ Speed: {total/(elapsed/60):.1f} articles/minute")
        print("="*70)
        
        # Final statistics - Read from single file
        print("\nüìä Final Statistics:")
        if os.path.exists(output_file):
            ticker_counts = {ticker: 0 for ticker in TICKERS}
            with open(output_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    ticker = row.get('ticker', '')
                    if ticker in ticker_counts:
                        ticker_counts[ticker] += 1
            
            total_articles = sum(ticker_counts.values())
            print(f"  Total articles: {total_articles}")
            for ticker in TICKERS:
                print(f"  {ticker}: {ticker_counts[ticker]:>5} articles")
            print(f"  File: {output_file}")
        
    except KeyboardInterrupt:
        print("\n[INFO] ‚ö†Ô∏è  Interrupted by user")
    except Exception as e:
        print(f"\n[ERROR] ‚ùå {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
