# ğŸ“‹ HÆ¯á»šNG DáºªN CHáº Y CRAWLER FULL 10 NÄ‚M

## ğŸ¯ Má»¥c tiÃªu
Crawl tin tá»©c cho 5 tickers (ACB, BID, VCB, MBB, FPT) tá»« 2015-2025, lÆ°u thÃ nh 5 file CSV riÃªng biá»‡t.

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i
- **5 file CSV**: `news_ACB_2015_2025.csv`, `news_BID_2015_2025.csv`, ...
- **250+ bÃ i/ticker/nÄƒm** = ~2,500-3,000 bÃ i/ticker
- **Tá»•ng cá»™ng**: ~12,000-15,000 articles
- **Thá»i gian**: 20-30 phÃºt

## ğŸš€ CÃCH 1: Cháº¡y trá»±c tiáº¿p (CÃ³ thá»ƒ theo dÃµi)

```bash
# 1. KÃ­ch hoáº¡t virtual environment
cd "/mnt/d/Ky 4/financial-news-sentiment-main/Source/recode"
source "/mnt/d/Ky 4/financial-news-sentiment-main/venv/bin/activate"

# 2. Cháº¡y crawler
python multi_source_crawler.py

# Khi Ä‘Æ°á»£c há»i "Overwrite all? (y/n):" â†’ nháº¥n 'y' vÃ  Enter
```

**Æ¯u Ä‘iá»ƒm**: Xem output trá»±c tiáº¿p, dá»… debug  
**NhÆ°á»£c Ä‘iá»ƒm**: Pháº£i giá»¯ terminal má»Ÿ

---

## ğŸŒ™ CÃCH 2: Cháº¡y background (Tá»± Ä‘á»™ng cháº¡y)

```bash
# 1. KÃ­ch hoáº¡t virtual environment vÃ  chuyá»ƒn thÆ° má»¥c
cd "/mnt/d/Ky 4/financial-news-sentiment-main/Source/recode"
source "/mnt/d/Ky 4/financial-news-sentiment-main/venv/bin/activate"

# 2. Cháº¡y trong background vá»›i nohup
nohup python multi_source_crawler.py > crawler_full.log 2>&1 << EOF &
y
EOF

# 3. Láº¥y PID (Process ID)
echo $! > crawler.pid
echo "Crawler started with PID: $(cat crawler.pid)"
```

**Æ¯u Ä‘iá»ƒm**: Cháº¡y background, cÃ³ thá»ƒ Ä‘Ã³ng terminal  
**NhÆ°á»£c Ä‘iá»ƒm**: KhÃ´ng tháº¥y output trá»±c tiáº¿p

### Theo dÃµi tiáº¿n Ä‘á»™:
```bash
# Xem log real-time
tail -f crawler_full.log

# Xem 50 dÃ²ng cuá»‘i
tail -50 crawler_full.log

# Xem tiáº¿n Ä‘á»™ (YEAR, articles saved)
tail -100 crawler_full.log | grep -E "(YEAR|âœ….*articles|SAVE)"

# Kiá»ƒm tra crawler cÃ²n cháº¡y khÃ´ng
ps aux | grep multi_source_crawler | grep -v grep
```

### Dá»«ng crawler (náº¿u cáº§n):
```bash
# Äá»c PID tá»« file
kill $(cat crawler.pid)

# Hoáº·c dá»«ng trá»±c tiáº¿p
pkill -f multi_source_crawler.py
```

---

## ğŸ“ˆ Theo dÃµi tiáº¿n Ä‘á»™ trong khi cháº¡y

### Kiá»ƒm tra sá»‘ lÆ°á»£ng bÃ i Ä‘Ã£ crawl:
```bash
# Äáº¿m sá»‘ dÃ²ng tá»«ng file (trá»« header)
wc -l news_*_2015_2025.csv

# Xem chi tiáº¿t tá»«ng ticker
for ticker in ACB BID VCB MBB FPT; do
    file="news_${ticker}_2015_2025.csv"
    if [ -f "$file" ]; then
        count=$(($(wc -l < "$file") - 1))
        echo "$ticker: $count articles"
    fi
done
```

### Kiá»ƒm tra tiáº¿n Ä‘á»™ theo nÄƒm:
```bash
# ACB - Ä‘áº¿m theo nÄƒm
for year in {2015..2025}; do
    count=$(grep -c "^$year-" news_ACB_2015_2025.csv 2>/dev/null || echo "0")
    echo "ACB $year: $count articles"
done
```

### Xem 10 bÃ i má»›i nháº¥t:
```bash
tail -10 news_ACB_2015_2025.csv
```

---

## ğŸ“Š Sau khi crawl xong

### 1. Kiá»ƒm tra káº¿t quáº£:
```bash
# Xem thá»‘ng kÃª cuá»‘i cÃ¹ng tá»« log
tail -100 crawler_full.log

# Hoáº·c táº¡o thá»‘ng kÃª má»›i
python << EOF
import csv

tickers = ["ACB", "BID", "VCB", "MBB", "FPT"]
print("="*60)
print("ğŸ“Š FINAL STATISTICS")
print("="*60)

total = 0
for ticker in tickers:
    filename = f"news_{ticker}_2015_2025.csv"
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            count = sum(1 for _ in f) - 1  # Subtract header
            total += count
            size_mb = os.path.getsize(filename) / (1024*1024)
            print(f"{ticker}: {count:>5} articles | {size_mb:>6.2f} MB")
    except:
        print(f"{ticker}: File not found")

print("="*60)
print(f"TOTAL: {total:>5} articles")
EOF
```

### 2. Validate dá»¯ liá»‡u:
```bash
# Kiá»ƒm tra file cÃ³ Ä‘Ãºng format khÃ´ng
head -5 news_ACB_2015_2025.csv

# Kiá»ƒm tra cÃ³ bÃ i trÃ¹ng khÃ´ng (so sÃ¡nh source column)
python << EOF
import csv

def check_duplicates(ticker):
    filename = f"news_{ticker}_2015_2025.csv"
    urls = set()
    duplicates = 0
    
    with open(filename, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            url = row['source']
            if url in urls:
                duplicates += 1
            urls.add(url)
    
    print(f"{ticker}: {len(urls)} unique URLs, {duplicates} duplicates")

for ticker in ["ACB", "BID", "VCB", "MBB", "FPT"]:
    check_duplicates(ticker)
EOF
```

---

## ğŸ”§ Xá»­ lÃ½ lá»—i

### Náº¿u crawler bá»‹ dá»«ng giá»¯a chá»«ng:
```bash
# Kiá»ƒm tra log Ä‘á»ƒ xem lá»—i gÃ¬
tail -100 crawler_full.log

# Cháº¡y láº¡i - crawler sáº½ há»i cÃ³ overwrite khÃ´ng
# Náº¿u muá»‘n tiáº¿p tá»¥c thÃªm vÃ o file cÅ©, cáº§n chá»‰nh code
```

### Náº¿u thiáº¿u packages:
```bash
pip install requests beautifulsoup4
```

### Náº¿u bá»‹ timeout:
- TÄƒng `REQUEST_DELAY` trong code (tá»« 0.2 â†’ 0.5)
- Giáº£m `MAX_WORKERS` (tá»« 5 â†’ 3)

---

## ğŸ’¾ Backup dá»¯ liá»‡u

```bash
# NÃ©n táº¥t cáº£ CSV files
tar -czf news_backup_$(date +%Y%m%d).tar.gz news_*_2015_2025.csv

# Copy sang thÆ° má»¥c khÃ¡c
mkdir -p ../backup
cp news_*_2015_2025.csv ../backup/
```

---

## ğŸ“ Notes

1. **Crawler sáº½ tá»± Ä‘á»™ng**:
   - Loáº¡i bá» duplicate URLs
   - Validate content (min 100 chars)
   - Filter theo ticker mentions trong content
   - Retry khi gáº·p lá»—i

2. **File format**:
   - Columns: `date`, `time`, `title`, `content`, `ticker`, `source`
   - Encoding: UTF-8
   - Newline: CRLF (Windows) hoáº·c LF (Linux)

3. **Thá»i gian Æ°á»›c tÃ­nh**:
   - 2015-2019: ~15-20 phÃºt (Ã­t tin hÆ¡n)
   - 2020-2025: ~10-15 phÃºt (nhiá»u tin hÆ¡n)
   - Total: 25-35 phÃºt

4. **Náº¿u muá»‘n crawl láº¡i 1 ticker**:
   ```bash
   # XÃ³a file cÅ©
   rm news_ACB_2015_2025.csv
   
   # Sá»­a code Ä‘á»ƒ chá»‰ crawl ACB
   # TICKERS = ["ACB"]
   
   # Cháº¡y láº¡i
   python multi_source_crawler.py
   ```

---

## âœ… CHECKLIST

TrÆ°á»›c khi cháº¡y:
- [ ] Virtual environment Ä‘Ã£ activate
- [ ] ThÆ° má»¥c Ä‘Ãºng: `/Source/recode`
- [ ] Internet connection á»•n Ä‘á»‹nh
- [ ] Äá»§ dung lÆ°á»£ng á»• Ä‘Ä©a (~500MB)

Trong khi cháº¡y:
- [ ] Theo dÃµi log Ä‘á»‹nh ká»³
- [ ] Kiá»ƒm tra file size tÄƒng
- [ ] Monitor memory usage (náº¿u cáº§n)

Sau khi xong:
- [ ] Validate 5 files CSV
- [ ] Check statistics
- [ ] Backup dá»¯ liá»‡u
- [ ] Test Ä‘á»c file báº±ng pandas

---

## ğŸ‰ Káº¿t quáº£ mong Ä‘á»£i

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:

```
news_ACB_2015_2025.csv  (~100MB, 2,500-3,000 articles)
news_BID_2015_2025.csv  (~80MB,  2,000-2,500 articles)  
news_VCB_2015_2025.csv  (~120MB, 3,000-3,500 articles)
news_MBB_2015_2025.csv  (~90MB,  2,000-2,500 articles)
news_FPT_2015_2025.csv  (~100MB, 2,500-3,000 articles)
```

**Tá»”NG**: ~500MB, 12,000-15,000 articles

Má»—i file sáºµn sÃ ng Ä‘á»ƒ:
- PhÃ¢n tÃ­ch sentiment
- Training models
- Time series analysis
- Text mining
